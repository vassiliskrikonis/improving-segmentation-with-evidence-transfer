{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evidence Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "FOLDS = 20\n",
    "wandb.init(project='evidence-transfer', config={\n",
    "    'unet_loss': 'categorical_crossentropy',\n",
    "    'connecting_unet_layer': 'expanding_block_64_conv2',\n",
    "    'q_loss': 'binary_crossentropy',\n",
    "    'q_lambda': 1.0,\n",
    "    'optimizer': 'sgd',\n",
    "    'learning_rate': 1e-4,\n",
    "    'momentum': 0.9,\n",
    "    'batch_size': 3,\n",
    "    'dataset': f'folds{FOLDS}',\n",
    "    'max_epochs': 100,\n",
    "    'baseline_model': '1f5s41d8',\n",
    "}, resume='allow')\n",
    "hparams = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.unet import create_unet\n",
    "from models.evitram import create_evidence_transfer_model\n",
    "import tensorflow as tf\n",
    "\n",
    "unet = create_unet()\n",
    "unet_weights = wandb.restore(\n",
    "    'model-best.h5',\n",
    "    run_path=f'vassilis_krikonis/unet-baseline/{hparams[\"baseline_model\"]}'\n",
    ")\n",
    "unet.load_weights(unet_weights.name)\n",
    "\n",
    "q_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(1, (1, 1), padding='same', activation='sigmoid'),\n",
    "], name='Q')\n",
    "\n",
    "evitram = create_evidence_transfer_model(\n",
    "    unet,\n",
    "    q_model,\n",
    "    'expanding_block_64_conv2',\n",
    "    loss_lambda=hparams['q_lambda']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.utils.plot_model(evitram, show_layer_names=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import CategoricalMeanIou\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "if hparams['optimizer'] == 'sgd':\n",
    "    optimizer = SGD(learning_rate=hparams['learning_rate'], momentum=hparams['momentum'])\n",
    "elif hparams['optimizer'] == 'adam':\n",
    "    optimizer = Adam(learning_rate=hparams['learning_rate'])\n",
    "else:\n",
    "    optimizer = hparams['optimizer']\n",
    "\n",
    "evitram.compile(optimizer=optimizer, metrics=[\n",
    "    [CategoricalMeanIou(num_classes=5), 'accuracy'],\n",
    "    ['accuracy']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /tmp/ds_cache/\n",
    "from datasets.skyline12 import Skyline12\n",
    "\n",
    "skyline12 = Skyline12('datasets/skyline12/data/')\n",
    "\n",
    "\n",
    "def split_outputs(x, y, z):\n",
    "    return x, (y, z)\n",
    "\n",
    "\n",
    "train_ds = skyline12 \\\n",
    "    .as_tf_dataset(FOLDS, subset='training', cache_dir='/tmp/ds_cache/') \\\n",
    "    .map(split_outputs)\n",
    "val_ds = skyline12 \\\n",
    "    .as_tf_dataset(FOLDS, subset='validation', cache_dir='/tmp/ds_cache/') \\\n",
    "    .map(split_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, _ = next(iter(val_ds.batch(3)))\n",
    "preds = evitram(batch_x, training=False)\n",
    "\n",
    "for x, y_pred, z_pred in zip(batch_x, *preds):\n",
    "    Skyline12.show_sample(x, [y_pred, z_pred], from_tensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "from utils import get_new_logdir\n",
    "from callbacks import LogEviTRAMImagesWandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "log_dir = get_new_logdir(root_dir='./logs')\n",
    "batch_size = hparams['batch_size']\n",
    "if wandb.run.resumed:\n",
    "    evitram.load_weights(wandb.restore('model-best.h5', replace=True).name)\n",
    "evitram.fit(\n",
    "    train_ds.batch(batch_size).prefetch(AUTOTUNE),\n",
    "    epochs=hparams['max_epochs'],\n",
    "    initial_epoch=wandb.run.step,\n",
    "    validation_data=val_ds.batch(batch_size).prefetch(AUTOTUNE),\n",
    "    callbacks=[\n",
    "        TensorBoard(\n",
    "            log_dir=log_dir,\n",
    "            histogram_freq=1,\n",
    "        ),\n",
    "        LogEviTRAMImagesWandb(next(iter(val_ds.batch(10)))),\n",
    "        WandbCallback(save_weights_only=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, _ = next(iter(val_ds.batch(3)))\n",
    "preds = evitram(batch_x, training=False)\n",
    "\n",
    "for x, y_pred, z_pred in zip(batch_x, *preds):\n",
    "    Skyline12.show_sample(x, [y_pred, z_pred], from_tensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
